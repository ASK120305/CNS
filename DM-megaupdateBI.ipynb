{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhWnQg88Jrfdt9l7BiO89X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASK120305/CNS/blob/main/DM-megaupdateBI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# This script is adapted for Google Colab.\n",
        "# It opens a file dialog to upload a CSV from the user's local machine,\n",
        "# then runs basic preprocessing and (optionally) saves the cleaned CSV.\n",
        "\n",
        "def upload_csv_from_colab():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"This helper requires Google Colab. Run this script in a Colab notebook.\"\n",
        "        ) from e\n",
        "\n",
        "    print(\"Please use the file chooser to upload a CSV file from your local device.\")\n",
        "    uploaded = files.upload()  # opens dialog\n",
        "\n",
        "    if not uploaded:\n",
        "        raise RuntimeError(\"No file uploaded.\")\n",
        "\n",
        "    # Take the first uploaded file\n",
        "    filename = next(iter(uploaded))\n",
        "    file_bytes = uploaded[filename]\n",
        "\n",
        "    # Try reading the CSV (auto-detect encoding if possible)\n",
        "    try:\n",
        "        df = pd.read_csv(io.BytesIO(file_bytes))\n",
        "    except Exception:\n",
        "        # fallback with latin1 encoding\n",
        "        df = pd.read_csv(io.BytesIO(file_bytes), encoding=\"latin1\")\n",
        "\n",
        "    return df, filename\n",
        "\n",
        "\n",
        "def basic_cleaning(df):\n",
        "    # Display initial info\n",
        "    print(\"Original data shape:\", df.shape)\n",
        "    try:\n",
        "        print(df.info())\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(df.head())\n",
        "\n",
        "    # 1. Drop duplicate rows\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # 2. Handle missing values\n",
        "    # Drop rows with all fields missing\n",
        "    df = df.dropna(how=\"all\")\n",
        "\n",
        "    # For columns with too many missing values, drop column (example threshold 70% missing)\n",
        "    missing_pct = df.isnull().mean()\n",
        "    cols_to_drop = missing_pct[missing_pct > 0.7].index\n",
        "    if len(cols_to_drop) > 0:\n",
        "        print(\"Dropping columns with >70% missing values:\", list(cols_to_drop))\n",
        "    df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    # For columns with moderate missing, fill with median (numeric) or mode (categorical)\n",
        "    for col in df.columns:\n",
        "        # numeric detection\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            if df[col].isnull().all():\n",
        "                # leave column as-is (all NaN)\n",
        "                continue\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "        else:\n",
        "            # for non-numeric, fill with mode if available, else with empty string\n",
        "            try:\n",
        "                mode_vals = df[col].mode()\n",
        "                if not mode_vals.empty:\n",
        "                    fill_val = mode_vals[0]\n",
        "                else:\n",
        "                    fill_val = \"\"\n",
        "            except Exception:\n",
        "                fill_val = \"\"\n",
        "            df[col] = df[col].fillna(fill_val)\n",
        "\n",
        "    # 3. General type conversion (example: convert 'date' columns to datetime)\n",
        "    for col in df.columns:\n",
        "        if \"date\" in col.lower():\n",
        "            # coerce errors -> NaT for unparsable entries\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "\n",
        "    # 4. Lowercase string columns and strip whitespace\n",
        "    for col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
        "        # convert to string type safely, skip NaNs\n",
        "        df[col] = df[col].astype(\"string\").str.lower().str.strip().replace(\"<nan>\", pd.NA)\n",
        "\n",
        "    # 5. (Optional) Remove outliers from numerical columns (using Z-score)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if numeric_cols:\n",
        "        try:\n",
        "            from scipy.stats import zscore\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                # zscore returns nan for constant columns; fillna with 0 so they are kept\n",
        "                col_values = df[col].copy()\n",
        "                # we compute zscore on the column, ignoring NaNs\n",
        "                zs = zscore(col_values.fillna(col_values.mean()))\n",
        "                mask = (np.abs(zs) < 3) | (col_values.isnull())\n",
        "                df = df[mask]\n",
        "        except Exception:\n",
        "            # fallback to manual z-score if scipy not available\n",
        "            for col in numeric_cols:\n",
        "                col_values = df[col]\n",
        "                if col_values.std(ddof=0) == 0 or np.isnan(col_values.std(ddof=0)):\n",
        "                    continue\n",
        "                z = (col_values - col_values.mean()) / col_values.std(ddof=0)\n",
        "                mask = (np.abs(z) < 3) | (col_values.isnull())\n",
        "                df = df[mask]\n",
        "\n",
        "    print(\"Data after cleaning:\", df.shape)\n",
        "    print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Upload CSV from local device (Colab file dialog)\n",
        "    df, original_filename = upload_csv_from_colab()\n",
        "\n",
        "    # Run basic cleaning\n",
        "    cleaned_df = basic_cleaning(df)\n",
        "\n",
        "    # Optionally save cleaned file back to the user's machine\n",
        "    try:\n",
        "        from google.colab import files\n",
        "\n",
        "        out_name = f\"cleaned_{original_filename}\"\n",
        "        cleaned_df.to_csv(out_name, index=False)\n",
        "        print(f\"Cleaned CSV saved as {out_name}. It will be downloaded to your local machine.\")\n",
        "        files.download(out_name)\n",
        "    except Exception:\n",
        "        # If download fails (e.g., not running in Colab) just print instruction\n",
        "        print(\"Finished cleaning. To save the dataframe, call cleaned_df.to_csv('cleaned.csv', index=False).\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ND2WoA4sSGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F5vLNIOfbuj9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# ---------- Step 1: Upload and load ----------\n",
        "print(\"Please upload the CSV file (e.g. 'data.csv' from the Kaggle breast cancer dataset).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No file uploaded. Exiting.\")\n",
        "    sys.exit(0)\n",
        "\n",
        "csv_path = next(iter(uploaded.keys()))\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"Loaded: {csv_path}\\n\")\n",
        "\n",
        "# ---------- Step 2: Clean & canonicalize ----------\n",
        "# Drop completely empty trailing column if present (Kaggle dataset has Unnamed: 32)\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "\n",
        "# Drop 'id' column - not useful for analysis\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "\n",
        "# Map diagnosis to Outcome for compatibility with older scripts (M -> 1 malignant, B -> 0 benign)\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "# ---------- Step 3: Basic inspection ----------\n",
        "print(\"First 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nDataFrame info:\")\n",
        "display(df.info())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# ---------- Step 4: Target (diagnosis / Outcome) overview ----------\n",
        "if 'diagnosis' in df.columns:\n",
        "    print(\"\\nDiagnosis value counts:\")\n",
        "    display(df['diagnosis'].value_counts())\n",
        "\n",
        "if 'Outcome' in df.columns:\n",
        "    print(\"\\nOutcome value counts (mapped):\")\n",
        "    display(df['Outcome'].value_counts())\n",
        "\n",
        "# ---------- Step 5: Statistical summary (with medians, IQR, and mode) ----------\n",
        "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "desc = df[num_cols].describe().T\n",
        "desc['median'] = df[num_cols].median()\n",
        "desc['IQR'] = df[num_cols].quantile(0.75) - df[num_cols].quantile(0.25)\n",
        "desc['mode'] = df[num_cols].mode().iloc[0]   # <-- Added line for mode\n",
        "print(\"\\nStatistical summary for numeric columns:\")\n",
        "display(desc[['count','mean','std','min','25%','median','75%','max','IQR','mode']])\n",
        "\n",
        "# ---------- Step 6: Correlation analysis (full but uncluttered) ----------\n",
        "corr = df[num_cols].corr()\n",
        "\n",
        "# Mask upper triangle for clarity\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "plt.figure(figsize=(14,12))\n",
        "sns.heatmap(corr, mask=mask, cmap='vlag', center=0, annot=False, fmt='.2f',\n",
        "            linewidths=0.5, cbar_kws={\"shrink\": .6})\n",
        "plt.title(\"Correlation Heatmap (full numeric features, upper triangle masked)\", fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- Step 7: Concise correlation heatmap for top features related to Outcome ----------\n",
        "if 'Outcome' in corr.columns:\n",
        "    # Identify features most correlated with Outcome (absolute correlation)\n",
        "    corr_with_outcome = corr['Outcome'].abs().sort_values(ascending=False)\n",
        "    corr_with_outcome = corr_with_outcome.drop('Outcome', errors='ignore')  # remove self\n",
        "    top_n = 12  # show concise matrix for top N features\n",
        "    top_features = corr_with_outcome.head(top_n).index.tolist()\n",
        "    concise_cols = top_features + ['Outcome']\n",
        "    concise_corr = df[concise_cols].corr()\n",
        "\n",
        "    # Mask upper triangle for neatness\n",
        "    mask2 = np.triu(np.ones_like(concise_corr, dtype=bool))\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(concise_corr, mask=mask2, cmap='vlag', center=0, annot=True, fmt='.2f',\n",
        "                annot_kws={\"size\":8}, linewidths=0.5, cbar_kws={\"shrink\": .6})\n",
        "    plt.title(f\"Concise Correlation (top {top_n} features by |corr| with Outcome)\", fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop features by absolute correlation with Outcome:\")\n",
        "    display(corr_with_outcome.head(top_n))\n",
        "else:\n",
        "    print(\"No 'Outcome' column present, skipping top-feature concise correlation heatmap.\")\n",
        "\n",
        "# ---------- Step 8: Distributions and boxplots for top features ----------\n",
        "# Choose top ~6 features (or fewer if dataset has fewer)\n",
        "if 'Outcome' in df.columns:\n",
        "    top_pairplot_n = 6\n",
        "    top_for_plots = corr_with_outcome.head(top_pairplot_n).index.tolist()\n",
        "else:\n",
        "    # fallback: choose first 6 numeric columns\n",
        "    top_for_plots = num_cols[:6]\n",
        "\n",
        "print(f\"\\nPlotting distributions and boxplots for: {top_for_plots}\")\n",
        "\n",
        "# Histograms / KDEs by diagnosis\n",
        "for col in top_for_plots:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    if 'diagnosis' in df.columns:\n",
        "        sns.kdeplot(data=df, x=col, hue='diagnosis', fill=True, common_norm=False, alpha=0.4)\n",
        "        plt.title(f\"{col} distribution by diagnosis\")\n",
        "    elif 'Outcome' in df.columns:\n",
        "        sns.kdeplot(data=df, x=col, hue='Outcome', fill=True, common_norm=False, alpha=0.4)\n",
        "        plt.title(f\"{col} distribution by Outcome\")\n",
        "    else:\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f\"{col} distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Boxplots grouped by diagnosis/outcome\n",
        "group_col = 'diagnosis' if 'diagnosis' in df.columns else ('Outcome' if 'Outcome' in df.columns else None)\n",
        "if group_col:\n",
        "    for col in top_for_plots:\n",
        "        plt.figure(figsize=(8,4))\n",
        "        sns.boxplot(x=group_col, y=col, data=df)\n",
        "        plt.title(f\"{col} by {group_col}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No group column (diagnosis/Outcome) found; skipping grouped boxplots.\")\n",
        "\n",
        "# ---------- Step 9: Pairplot for the top few features (keeps it small to avoid clutter) ----------\n",
        "pairplot_features = top_for_plots[:6]  # at most 6\n",
        "if len(pairplot_features) >= 2:\n",
        "    try:\n",
        "        hue_arg = 'diagnosis' if 'diagnosis' in df.columns else ('Outcome' if 'Outcome' in df.columns else None)\n",
        "        sns.pairplot(df[pairplot_features + ([hue_arg] if hue_arg else [])], hue=hue_arg, palette='Set2', diag_kind='kde', corner=True)\n",
        "        plt.suptitle(\"Pairplot (top features)\", y=1.02)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"Pairplot failed (too many points or another issue):\", e)\n",
        "else:\n",
        "    print(\"Not enough features for pairplot.\")\n",
        "\n",
        "# ---------- Step 10: Quick takeaways printed ----------\n",
        "\n",
        "print(\"\\nQuick takeaways:\")\n",
        "if 'Outcome' in df.columns:\n",
        "    # Print top 6 positive/negative correlations with outcome\n",
        "    signed_corr = df[num_cols].corr()['Outcome'].drop('Outcome').sort_values()\n",
        "    print(\"- Features most negatively correlated with Outcome (benign-associated):\")\n",
        "    display(signed_corr.head(6))\n",
        "    print(\"- Features most positively correlated with Outcome (malignant-associated):\")\n",
        "    display(signed_corr.tail(6))\n",
        "else:\n",
        "    print(\"- No 'Outcome' to compute feature associations with. Inspect diagnosis column instead.\")\n",
        "\n",
        "print(\"\"\"\n",
        "Notes & next steps:\n",
        "- The full heatmap (masked) shows all numeric correlations but can be cluttered for 30+ features.\n",
        "- The concise matrix (above) shows only the top features most correlated with Outcome and is annotated for readability.\n",
        "- You can optionally run feature selection, PCA, or build classification models (LogisticRegression / RandomForest) using these top features.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree classifier on Kaggle Breast Cancer (Wisconsin) dataset\n",
        "# Run this in Google Colab. Upload the CSV when prompted.\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# -------------------\n",
        "# Upload / load CSV\n",
        "# -------------------\n",
        "print(\"Please upload the Kaggle breast cancer CSV (e.g. 'data.csv').\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Exiting.\")\n",
        "\n",
        "csv_path = next(iter(uploaded.keys()))\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"Loaded file: {csv_path}\\n\")\n",
        "\n",
        "# -------------------\n",
        "# Clean / canonicalize\n",
        "# -------------------\n",
        "# Drop trailing empty column if present\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "\n",
        "# Drop id column (not informative)\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "\n",
        "# Map diagnosis to Outcome: M -> 1 (malignant), B -> 0 (benign)\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' to 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# -------------------\n",
        "# Feature selection\n",
        "# -------------------\n",
        "features = [\n",
        "    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean'\n",
        "]\n",
        "\n",
        "existing_features = [f for f in features if f in df.columns]\n",
        "if len(existing_features) < 4:\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [c for c in numeric_cols if c != 'Outcome']\n",
        "    existing_features = numeric_cols[:8]\n",
        "    print(f\"Using fallback numeric features: {existing_features}\")\n",
        "\n",
        "features = existing_features\n",
        "\n",
        "# Ensure Outcome exists\n",
        "if 'Outcome' not in df.columns:\n",
        "    raise SystemExit(\"No 'Outcome' or 'diagnosis' column found in the uploaded file. Exiting.\")\n",
        "\n",
        "# Drop rows with missing values in features/target\n",
        "df_model = df.dropna(subset=features + ['Outcome']).copy()\n",
        "\n",
        "# Prepare X, y\n",
        "y = df_model['Outcome'].astype(int)\n",
        "X = df_model[features]\n",
        "\n",
        "# -------------------\n",
        "# Train/Test split (deterministic, stratified)\n",
        "# -------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# Decision Tree classifier\n",
        "# -------------------\n",
        "dt_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"\\nAccuracy of Decision Tree (max_depth=3): {accuracy_dt:.4f}\\n\")\n",
        "print(\"Classification report - Decision Tree:\")\n",
        "print(classification_report(y_test, y_pred_dt, labels=[0,1], target_names=['Benign','Malignant'], zero_division=0))\n",
        "\n",
        "# Plot Decision Tree\n",
        "plt.figure(figsize=(16,9))\n",
        "plot_tree(dt_classifier, filled=True, feature_names=features, class_names=['Benign','Malignant'], fontsize=10)\n",
        "plt.title(\"Decision Tree (max_depth=3) - Breast Cancer\")\n",
        "plt.show()\n",
        "\n",
        "# Feature importances\n",
        "importances = dt_classifier.feature_importances_\n",
        "if np.any(importances):\n",
        "    feat_imp = pd.Series(importances, index=features).sort_values(ascending=True)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    feat_imp.plot(kind='barh', color='C1')\n",
        "    plt.title(\"Decision Tree Feature Importances\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Confusion matrix - Decision Tree\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt, labels=[0,1])\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign','Malignant'], yticklabels=['Benign','Malignant'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.show()\n",
        "\n",
        "print(\"Done. Decision Tree model trained and evaluated on features:\", features)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1ApN4Rq9gVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naive Bayes classifier on Kaggle Breast Cancer (Wisconsin) dataset\n",
        "# Run this in Google Colab. Upload the same CSV when prompted.\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# -------------------\n",
        "# Upload / load CSV\n",
        "# -------------------\n",
        "print(\"Please upload the Kaggle breast cancer CSV (e.g. 'data.csv').\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Exiting.\")\n",
        "\n",
        "csv_path = next(iter(uploaded.keys()))\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"Loaded file: {csv_path}\\n\")\n",
        "\n",
        "# -------------------\n",
        "# Clean / canonicalize\n",
        "# -------------------\n",
        "# Drop trailing empty column if present\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "\n",
        "# Drop id column (not informative)\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "\n",
        "# Map diagnosis to Outcome: M -> 1 (malignant), B -> 0 (benign)\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' to 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# -------------------\n",
        "# Feature selection (must match the Decision Tree file for comparability)\n",
        "# -------------------\n",
        "features = [\n",
        "    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean'\n",
        "]\n",
        "\n",
        "existing_features = [f for f in features if f in df.columns]\n",
        "if len(existing_features) < 4:\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [c for c in numeric_cols if c != 'Outcome']\n",
        "    existing_features = numeric_cols[:8]\n",
        "    print(f\"Using fallback numeric features: {existing_features}\")\n",
        "\n",
        "features = existing_features\n",
        "\n",
        "# Ensure Outcome exists\n",
        "if 'Outcome' not in df.columns:\n",
        "    raise SystemExit(\"No 'Outcome' or 'diagnosis' column found in the uploaded file. Exiting.\")\n",
        "\n",
        "# Drop rows with missing values in features/target\n",
        "df_model = df.dropna(subset=features + ['Outcome']).copy()\n",
        "\n",
        "# Prepare X, y\n",
        "y = df_model['Outcome'].astype(int)\n",
        "X = df_model[features]\n",
        "\n",
        "# -------------------\n",
        "# Train/Test split (must use same deterministic params as Decision Tree script)\n",
        "# -------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# Gaussian Naive Bayes classifier\n",
        "# -------------------\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"\\nAccuracy of Gaussian Naive Bayes: {accuracy_nb:.4f}\\n\")\n",
        "print(\"Classification report - Naive Bayes:\")\n",
        "print(classification_report(y_test, y_pred_nb, labels=[0,1], target_names=['Benign','Malignant'], zero_division=0))\n",
        "\n",
        "# Confusion matrix - Naive Bayes\n",
        "cm_nb = confusion_matrix(y_test, y_pred_nb, labels=[0,1])\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign','Malignant'], yticklabels=['Benign','Malignant'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# Optional: probability distributions for the positive class (Malignant)\n",
        "if hasattr(nb_classifier, \"predict_proba\"):\n",
        "    y_prob = nb_classifier.predict_proba(X_test)[:, 1]\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.histplot(y_prob, bins=20, kde=True, color='C2')\n",
        "    plt.title(\"Predicted probability distribution (Malignant) - Naive Bayes\")\n",
        "    plt.xlabel(\"Predicted probability\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Done. GaussianNB model trained and evaluated on features:\", features)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GH347DOAhUkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agglomerative Clustering (Colab-ready) for uploaded dataset (Breast Cancer / Diabetes / general CSV)\n",
        "# - Upload your CSV when prompted in Colab (e.g. the Kaggle breast-cancer-wisconsin-data data.csv).\n",
        "# - The script will canonicalize the breast-cancer dataset if detected (drop id/Unnamed: 32, map diagnosis -> Outcome).\n",
        "# - For diabetes-like datasets it will also optionally replace medically-invalid zeros (Glucose, BloodPressure, SkinThickness, Insulin, BMI).\n",
        "# - Produces a dendrogram (sampled), a cluster scatter using PCA (2 components), cluster statistics, silhouette score,\n",
        "#   and saves a clustered CSV that you can download from Colab.\n",
        "#\n",
        "# Run this as a single cell in Google Colab.\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import io\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# -------------------------\n",
        "# 1) Upload & load CSV\n",
        "# -------------------------\n",
        "print(\"Please upload your CSV file (e.g. Kaggle breast-cancer data.csv).\")\n",
        "uploaded = files.upload()  # opens Colab file picker\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No file uploaded. Exiting.\")\n",
        "    raise SystemExit(\"No file uploaded\")\n",
        "\n",
        "csv_name = next(iter(uploaded.keys()))\n",
        "print(f\"Loaded file: {csv_name}\")\n",
        "# Read into DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[csv_name]))\n",
        "except Exception as e:\n",
        "    print(\"Failed to read uploaded CSV:\", e)\n",
        "    raise\n",
        "\n",
        "print(f\"Initial shape: {df.shape}\")\n",
        "print(\"Columns detected:\", df.columns.tolist())\n",
        "\n",
        "# -------------------------\n",
        "# 2) Dataset-specific cleaning\n",
        "# -------------------------\n",
        "# Handle common Kaggle breast-cancer dataset quirks\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "    print(\"Dropped column: Unnamed: 32\")\n",
        "\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "    print(\"Dropped column: id\")\n",
        "\n",
        "# Map diagnosis -> Outcome for compatibility\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' to 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# If this is the diabetes dataset (or similar) handle medically-invalid zeros\n",
        "zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "zero_cols_present = [c for c in zero_not_allowed if c in df.columns]\n",
        "if zero_cols_present:\n",
        "    print(\"Replacing medically-invalid zeros with column means for:\", zero_cols_present)\n",
        "    for col in zero_cols_present:\n",
        "        df[col] = df[col].replace(0, np.nan)\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Feature selection\n",
        "# -------------------------\n",
        "# Use numeric columns only (exclude Outcome if present)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Outcome' in numeric_cols:\n",
        "    numeric_cols.remove('Outcome')\n",
        "\n",
        "if len(numeric_cols) < 2:\n",
        "    raise SystemExit(\"Need at least two numeric features for clustering. Found: \" + str(numeric_cols))\n",
        "\n",
        "print(f\"Numeric features used for clustering ({len(numeric_cols)}): {numeric_cols}\")\n",
        "\n",
        "X = df[numeric_cols].copy()\n",
        "\n",
        "# Optionally drop columns with very low variance (near-constant) to reduce noise\n",
        "var_threshold = 1e-6\n",
        "low_var = X.var().loc[lambda s: s <= var_threshold].index.tolist()\n",
        "if low_var:\n",
        "    X = X.drop(columns=low_var)\n",
        "    print(\"Dropped near-constant columns:\", low_var)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Scale features\n",
        "# -------------------------\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Data scaled. Shape:\", X_scaled.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Dendrogram (sampled for clarity)\n",
        "# -------------------------\n",
        "sample_size = min(60, X_scaled.shape[0])  # keep sample small for clear dendrogram\n",
        "sample_indices = np.random.choice(X_scaled.shape[0], sample_size, replace=False)\n",
        "X_sample = X_scaled[sample_indices]\n",
        "\n",
        "print(f\"\\nGenerating dendrogram using a sample of {sample_size} rows...\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "try:\n",
        "    linkage_matrix = linkage(X_sample, method='ward')\n",
        "    dendrogram(linkage_matrix, truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=8)\n",
        "    plt.title(\"Dendrogram (sampled) - Ward linkage\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Sample index (truncated)\")\n",
        "    plt.ylabel(\"Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Dendrogram generation failed:\", e)\n",
        "\n",
        "# -------------------------\n",
        "# 6) Agglomerative clustering\n",
        "# -------------------------\n",
        "n_clusters = 3  # you can change this\n",
        "print(f\"\\nApplying AgglomerativeClustering with n_clusters={n_clusters} (ward linkage).\")\n",
        "agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "labels = agg.fit_predict(X_scaled)\n",
        "df['Cluster'] = labels\n",
        "\n",
        "# -------------------------\n",
        "# 7) Visualization: PCA scatter for first two components (clear & concise)\n",
        "# -------------------------\n",
        "print(\"Creating 2D PCA scatter plot of clusters (for visualization).\")\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pc = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "palette = sns.color_palette(\"tab10\", n_colors=max(3, n_clusters))\n",
        "for i in range(n_clusters):\n",
        "    mask = labels == i\n",
        "    plt.scatter(pc[mask, 0], pc[mask, 1], s=60, alpha=0.75, label=f\"Cluster {i}\", edgecolor='k')\n",
        "plt.xlabel(\"PC1 (%.1f%% var)\" % (pca.explained_variance_ratio_[0]*100))\n",
        "plt.ylabel(\"PC2 (%.1f%% var)\" % (pca.explained_variance_ratio_[1]*100))\n",
        "plt.title(f\"Agglomerative Clustering (n={n_clusters}) projected to 2 PC\", fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 8) Cluster analysis & silhouette\n",
        "# -------------------------\n",
        "print(\"\\n=== Cluster distribution ===\")\n",
        "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
        "print(cluster_counts.to_string())\n",
        "\n",
        "print(\"\\n=== Cluster means (selected features) ===\")\n",
        "for i in range(n_clusters):\n",
        "    subset = df[df['Cluster'] == i][numeric_cols]\n",
        "    if len(subset) > 0:\n",
        "        print(f\"\\nCluster {i} (n={len(subset)}):\")\n",
        "        display(subset.mean().round(3))\n",
        "\n",
        "# Silhouette score (only if more than 1 cluster and fewer clusters than samples)\n",
        "silhouette_avg = None\n",
        "try:\n",
        "    if len(set(labels)) > 1 and X_scaled.shape[0] > len(set(labels)):\n",
        "        silhouette_avg = silhouette_score(X_scaled, labels)\n",
        "        print(f\"\\nSilhouette Score: {silhouette_avg:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nSilhouette Score not computed (need >1 cluster and more samples than clusters).\")\n",
        "except Exception as e:\n",
        "    print(\"Silhouette score calculation error:\", e)\n",
        "\n",
        "# -------------------------\n",
        "# 9) Save clustered dataset and provide download link\n",
        "# -------------------------\n",
        "out_name = f\"clustered_{csv_name}\"\n",
        "try:\n",
        "    df.to_csv(out_name, index=False)\n",
        "    print(f\"\\nClustered dataset saved as: {out_name}\")\n",
        "    # Make file available for download in Colab\n",
        "    files.download(out_name)\n",
        "except Exception as e:\n",
        "    print(\"Could not save or download clustered CSV:\", e)\n",
        "\n",
        "# -------------------------\n",
        "# 10) If Outcome present: cross-tab\n",
        "# -------------------------\n",
        "if 'Outcome' in df.columns or 'diagnosis' in df.columns:\n",
        "    # Prefer numeric Outcome; if not present, use diagnosis\n",
        "    group_col = 'Outcome' if 'Outcome' in df.columns else 'diagnosis'\n",
        "    try:\n",
        "        print(f\"\\nCluster vs {group_col} cross-tab:\")\n",
        "        ct = pd.crosstab(df['Cluster'], df[group_col], margins=True)\n",
        "        display(ct)\n",
        "    except Exception as e:\n",
        "        print(\"Error computing cluster vs outcome cross-tab:\", e)\n",
        "\n",
        "print(\"\\nClustering complete. Adjust `n_clusters` or feature set to explore other clusterings.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zuu03Ty5j6VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN clustering (Colab-ready) for uploaded dataset (Breast Cancer / Diabetes / general CSV)\n",
        "# - Paste this cell into a Google Colab notebook and run.\n",
        "# - Upload your CSV when prompted (e.g. Kaggle breast-cancer-wisconsin-data data.csv).\n",
        "# - The script canonicalizes the breast-cancer CSV (drops id/Unnamed: 32, maps diagnosis->Outcome),\n",
        "#   optionally replaces medically-invalid zeros for diabetes-like datasets, runs DBSCAN on scaled numeric features,\n",
        "#   visualizes clusters on a 2D PCA projection, computes silhouette score (excluding noise), and downloads the clustered CSV.\n",
        "#\n",
        "# Usage: run the cell, upload the CSV, and inspect the printed outputs & plots.\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# -------------------------\n",
        "# User-tunable DBSCAN params\n",
        "# -------------------------\n",
        "# You can change these before running the cell again.\n",
        "EPS = 0.5\n",
        "MIN_SAMPLES = 5\n",
        "N_PCA_COMPONENTS = 2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# -------------------------\n",
        "# 1) Upload & load CSV\n",
        "# -------------------------\n",
        "print(\"Please upload your CSV file (e.g. Kaggle breast-cancer data.csv).\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Exiting.\")\n",
        "\n",
        "csv_name = next(iter(uploaded.keys()))\n",
        "print(f\"Loaded file: {csv_name}\")\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[csv_name]))\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"Failed to read CSV: {e}\")\n",
        "\n",
        "print(f\"Initial shape: {df.shape}\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# -------------------------\n",
        "# 2) Dataset-specific cleaning\n",
        "# -------------------------\n",
        "# Common Kaggle breast-cancer file quirks\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "    print(\"Dropped column: Unnamed: 32\")\n",
        "\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "    print(\"Dropped column: id\")\n",
        "\n",
        "# Map diagnosis -> Outcome for compatibility\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' to 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# Replace medically-invalid zeros if diabetes-like columns exist\n",
        "zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "zero_cols_present = [c for c in zero_not_allowed if c in df.columns]\n",
        "if zero_cols_present:\n",
        "    print(\"Replacing medically-invalid zeros with column means for:\", zero_cols_present)\n",
        "    for c in zero_cols_present:\n",
        "        df[c] = df[c].replace(0, np.nan)\n",
        "        df[c].fillna(df[c].mean(), inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Feature selection (numeric features)\n",
        "# -------------------------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Outcome' in numeric_cols:\n",
        "    numeric_cols.remove('Outcome')\n",
        "\n",
        "if len(numeric_cols) < 2:\n",
        "    raise SystemExit(f\"Need at least 2 numeric features for clustering. Found: {numeric_cols}\")\n",
        "\n",
        "print(f\"Numeric features used for clustering ({len(numeric_cols)}): {numeric_cols}\")\n",
        "\n",
        "X = df[numeric_cols].copy()\n",
        "\n",
        "# Optionally drop near-constant columns\n",
        "var_threshold = 1e-8\n",
        "low_var_cols = X.var().loc[lambda s: s <= var_threshold].index.tolist()\n",
        "if low_var_cols:\n",
        "    print(\"Dropping near-constant columns:\", low_var_cols)\n",
        "    X = X.drop(columns=low_var_cols)\n",
        "    numeric_cols = [c for c in numeric_cols if c not in low_var_cols]\n",
        "\n",
        "# -------------------------\n",
        "# 4) Scale features\n",
        "# -------------------------\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Data scaled. Shape:\", X_scaled.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 5) PCA for 2D visualization\n",
        "# -------------------------\n",
        "pca = PCA(n_components=N_PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
        "pc = pca.fit_transform(X_scaled)\n",
        "explained = pca.explained_variance_ratio_\n",
        "\n",
        "# -------------------------\n",
        "# 6) Run DBSCAN\n",
        "# -------------------------\n",
        "print(f\"\\nRunning DBSCAN with eps={EPS}, min_samples={MIN_SAMPLES} ...\")\n",
        "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\n",
        "labels = db.fit_predict(X_scaled)\n",
        "df['Cluster'] = labels\n",
        "\n",
        "unique_labels = set(labels)\n",
        "n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
        "n_noise = int((labels == -1).sum())\n",
        "\n",
        "print(f\"Clusters found (excluding noise): {n_clusters}\")\n",
        "print(f\"Noise points: {n_noise}\")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Visualization (PCA scatter)\n",
        "# -------------------------\n",
        "plt.figure(figsize=(10, 7))\n",
        "palette = sns.color_palette(\"tab10\", n_colors=max(3, len(unique_labels)))\n",
        "for lbl in sorted(unique_labels):\n",
        "    mask = labels == lbl\n",
        "    if lbl == -1:\n",
        "        # noise as gray x\n",
        "        plt.scatter(pc[mask, 0], pc[mask, 1], c='lightgray', marker='x', s=50, alpha=0.6, label='Noise')\n",
        "    else:\n",
        "        color = palette[lbl % len(palette)]\n",
        "        plt.scatter(pc[mask, 0], pc[mask, 1], c=[color], s=60, alpha=0.8, edgecolor='k', label=f'Cluster {lbl}')\n",
        "\n",
        "plt.xlabel(f\"PC1 ({explained[0]*100:.1f}% var)\")\n",
        "plt.ylabel(f\"PC2 ({explained[1]*100:.1f}% var)\" if len(explained) > 1 else \"PC2\")\n",
        "plt.title(f\"DBSCAN Clustering (eps={EPS}, min_samples={MIN_SAMPLES}) projected to 2 PC\", fontsize=14, fontweight='bold')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 8) Cluster counts & summary stats\n",
        "# -------------------------\n",
        "print(\"\\n=== Cluster distribution ===\")\n",
        "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
        "print(cluster_counts.to_string())\n",
        "\n",
        "print(\"\\n=== Cluster means (numeric features) ===\")\n",
        "for lbl in sorted(unique_labels):\n",
        "    subset = df[df['Cluster'] == lbl][numeric_cols]\n",
        "    if len(subset) > 0:\n",
        "        print(f\"\\nCluster {lbl} (n={len(subset)}):\")\n",
        "        display(subset.mean().round(3))\n",
        "\n",
        "# -------------------------\n",
        "# 9) Silhouette score (exclude noise)\n",
        "# -------------------------\n",
        "silhouette_avg = None\n",
        "try:\n",
        "    if n_clusters > 1:\n",
        "        mask_non_noise = labels != -1\n",
        "        if mask_non_noise.sum() > 1:\n",
        "            silhouette_avg = silhouette_score(X_scaled[mask_non_noise], labels[mask_non_noise])\n",
        "            print(f\"\\nSilhouette Score (excluding noise): {silhouette_avg:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNot enough non-noise points for silhouette score\")\n",
        "    else:\n",
        "        print(\"\\nNot enough clusters for silhouette score (need >1 cluster)\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nSilhouette score error: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# 10) Save clustered CSV and provide download\n",
        "# -------------------------\n",
        "out_name = f\"clustered_{csv_name}\"\n",
        "try:\n",
        "    df.to_csv(out_name, index=False)\n",
        "    print(f\"\\nClustered dataset saved as: {out_name}\")\n",
        "    files.download(out_name)\n",
        "except Exception as e:\n",
        "    print(\"Could not save or download clustered CSV:\", e)\n",
        "\n",
        "# -------------------------\n",
        "# 11) If Outcome present: cross-tab\n",
        "# -------------------------\n",
        "if 'Outcome' in df.columns or 'diagnosis' in df.columns:\n",
        "    group_col = 'Outcome' if 'Outcome' in df.columns else 'diagnosis'\n",
        "    try:\n",
        "        print(f\"\\nCluster vs {group_col} cross-tab:\")\n",
        "        ct = pd.crosstab(df['Cluster'], df[group_col], margins=True)\n",
        "        display(ct)\n",
        "    except Exception as e:\n",
        "        print(\"Error computing cross-tab:\", e)\n",
        "\n",
        "# -------------------------\n",
        "# 12) Parameter tuning suggestions\n",
        "# -------------------------\n",
        "print(\"\\n=== Parameter tuning suggestions ===\")\n",
        "print(f\"Current parameters: eps={EPS}, min_samples={MIN_SAMPLES}\")\n",
        "print(f\"Results: {n_clusters} clusters, {n_noise} noise points\")\n",
        "if n_clusters == 0:\n",
        "    print(\"Suggestion: Try decreasing eps to allow DBSCAN to discover tighter clusters, or increase min_samples if too many small clusters.\")\n",
        "elif n_noise > len(X_scaled) * 0.1:\n",
        "    print(\"Suggestion: Consider increasing eps or decreasing min_samples to reduce noise points.\")\n",
        "elif n_clusters == 1:\n",
        "    print(\"Suggestion: Decrease eps to attempt to split the data into multiple clusters.\")\n",
        "else:\n",
        "    print(\"Suggestion: Try varying eps in a small range around the current value and tune min_samples to balance cluster count vs noise.\")\n",
        "\n",
        "print(\"\\nDBSCAN clustering complete.\")"
      ],
      "metadata": {
        "id": "U4upKSmFka7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means Clustering (Colab-ready) for uploaded dataset (Breast Cancer / Diabetes / general CSV)\n",
        "# - Run this cell in Google Colab. Upload your CSV when prompted (e.g. Kaggle breast-cancer data.csv).\n",
        "# - The script canonicalizes common breast-cancer CSV quirks (drops id/Unnamed: 32, maps diagnosis -> Outcome),\n",
        "#   optionally replaces medically-invalid zeros for diabetes-like datasets, runs KMeans, shows the Elbow plot,\n",
        "#   visualizes clusters in 2D using PCA, computes silhouette score, and downloads the clustered CSV.\n",
        "#\n",
        "# Usage: run the cell, upload your CSV, adjust CHOSEN_K if you want a different number of clusters.\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "# -----------------------\n",
        "# User-tunable parameters\n",
        "# -----------------------\n",
        "K_RANGE = range(1, 11)    # range to evaluate for elbow method\n",
        "CHOSEN_K = 3              # default k to fit after elbow (change if desired)\n",
        "N_INIT = 10               # KMeans n_init\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# -----------------------\n",
        "# 1) Upload & load CSV\n",
        "# -----------------------\n",
        "print(\"Please upload your CSV file (e.g. Kaggle breast-cancer data.csv).\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Exiting.\")\n",
        "\n",
        "csv_name = next(iter(uploaded.keys()))\n",
        "print(f\"Loaded file: {csv_name}\")\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[csv_name]))\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"Failed to read CSV: {e}\")\n",
        "\n",
        "print(f\"Initial shape: {df.shape}\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# -----------------------\n",
        "# 2) Canonicalize dataset\n",
        "# -----------------------\n",
        "# Drop common Kaggle breast-cancer quirks\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "    print(\"Dropped column: Unnamed: 32\")\n",
        "\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "    print(\"Dropped column: id\")\n",
        "\n",
        "# Map diagnosis -> Outcome for compatibility\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' to numeric 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# Replace medically-invalid zeros if diabetes-like columns exist\n",
        "zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "zero_cols_present = [c for c in zero_not_allowed if c in df.columns]\n",
        "if zero_cols_present:\n",
        "    print(\"Replacing medically-invalid zeros with column means for:\", zero_cols_present)\n",
        "    for c in zero_cols_present:\n",
        "        df[c] = df[c].replace(0, np.nan)\n",
        "        df[c].fillna(df[c].mean(), inplace=True)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Feature selection\n",
        "# -----------------------\n",
        "# Use numeric features only for clustering; exclude Outcome if present\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Outcome' in numeric_cols:\n",
        "    numeric_cols.remove('Outcome')\n",
        "\n",
        "if len(numeric_cols) < 2:\n",
        "    raise SystemExit(\"Need at least two numeric features for clustering. Found: \" + str(numeric_cols))\n",
        "\n",
        "print(f\"Numeric features used for clustering ({len(numeric_cols)}): {numeric_cols}\")\n",
        "\n",
        "X = df[numeric_cols].copy()\n",
        "\n",
        "# Optionally drop near-constant columns\n",
        "low_var_thresh = 1e-8\n",
        "low_var_cols = X.var().loc[lambda s: s <= low_var_thresh].index.tolist()\n",
        "if low_var_cols:\n",
        "    print(\"Dropping near-constant columns:\", low_var_cols)\n",
        "    X = X.drop(columns=low_var_cols)\n",
        "    numeric_cols = [c for c in numeric_cols if c not in low_var_cols]\n",
        "\n",
        "# -----------------------\n",
        "# 4) Scale features\n",
        "# -----------------------\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Data scaled. Shape:\", X_scaled.shape)\n",
        "\n",
        "# -----------------------\n",
        "# 5) Elbow method (WCSS) to suggest k\n",
        "# -----------------------\n",
        "wcss = []\n",
        "print(\"\\nComputing WCSS for K in\", list(K_RANGE))\n",
        "for k in K_RANGE:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=N_INIT)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# -----------------------\n",
        "# 6) PCA for visualization\n",
        "# -----------------------\n",
        "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "pc = pca.fit_transform(X_scaled)\n",
        "explained = pca.explained_variance_ratio_\n",
        "\n",
        "# -----------------------\n",
        "# 7) Fit KMeans with chosen k and get labels\n",
        "# -----------------------\n",
        "chosen_k = CHOSEN_K\n",
        "if chosen_k < 1 or chosen_k > 50:\n",
        "    chosen_k = 3\n",
        "    print(f\"Invalid CHOSEN_K, defaulting to {chosen_k}\")\n",
        "\n",
        "kmeans_final = KMeans(n_clusters=chosen_k, random_state=RANDOM_STATE, n_init=N_INIT)\n",
        "labels = kmeans_final.fit_predict(X_scaled)\n",
        "df['Cluster'] = labels\n",
        "\n",
        "# Project centroids into PCA space for plotting\n",
        "centroids_scaled = kmeans_final.cluster_centers_\n",
        "centroids_pca = pca.transform(centroids_scaled)\n",
        "\n",
        "# -----------------------\n",
        "# 8) Plot Elbow (left) and PCA cluster scatter (right)\n",
        "# -----------------------\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Elbow plot\n",
        "axes[0].plot(list(K_RANGE), wcss, 'o-b', linewidth=2, markersize=6)\n",
        "axes[0].set_xlabel(\"Number of clusters (k)\")\n",
        "axes[0].set_ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
        "axes[0].set_title(\"Elbow Method for Optimal k\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].axvline(chosen_k, color='gray', linestyle='--', label=f'chosen k={chosen_k}')\n",
        "axes[0].legend()\n",
        "\n",
        "# PCA scatter with clusters\n",
        "palette = sns.color_palette(\"tab10\", n_colors=max(3, chosen_k))\n",
        "for lbl in sorted(set(labels)):\n",
        "    mask = labels == lbl\n",
        "    axes[1].scatter(pc[mask, 0], pc[mask, 1], s=60, alpha=0.75, edgecolor='k', label=f'Cluster {lbl}')\n",
        "# plot centroids\n",
        "axes[1].scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "axes[1].set_xlabel(f\"PC1 ({explained[0]*100:.1f}% var)\")\n",
        "axes[1].set_ylabel(f\"PC2 ({explained[1]*100:.1f}% var)\")\n",
        "axes[1].set_title(f\"K-Means Clustering (k={chosen_k}) projected to 2 PC\")\n",
        "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 9) Cluster evaluation & summaries\n",
        "# -----------------------\n",
        "n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(f\"\\nClusters found: {n_clusters_found} (k requested: {chosen_k})\")\n",
        "print(\"Cluster distribution:\")\n",
        "print(df['Cluster'].value_counts().sort_index().to_string())\n",
        "\n",
        "print(\"\\nCluster means (original feature space):\")\n",
        "for lbl in sorted(set(labels)):\n",
        "    subset = df[df['Cluster'] == lbl][numeric_cols]\n",
        "    if len(subset) > 0:\n",
        "        print(f\"\\nCluster {lbl} (n={len(subset)}):\")\n",
        "        display(subset.mean().round(3))\n",
        "\n",
        "# Silhouette score (only valid if more than 1 cluster)\n",
        "try:\n",
        "    if len(set(labels)) > 1:\n",
        "        sil = silhouette_score(X_scaled, labels)\n",
        "        print(f\"\\nSilhouette Score: {sil:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nSilhouette Score: not applicable (only one cluster present).\")\n",
        "except Exception as e:\n",
        "    print(\"\\nSilhouette score calculation error:\", e)\n",
        "\n",
        "# -----------------------\n",
        "# 10) Save clustered CSV and provide download\n",
        "# -----------------------\n",
        "out_name = f\"clustered_{csv_name}\"\n",
        "try:\n",
        "    df.to_csv(out_name, index=False)\n",
        "    print(f\"\\nClustered dataset saved as: {out_name}\")\n",
        "    files.download(out_name)\n",
        "except Exception as e:\n",
        "    print(\"Could not save or download clustered CSV:\", e)\n",
        "\n",
        "# -----------------------\n",
        "# 11) If Outcome present: cross-tab\n",
        "# -----------------------\n",
        "if 'Outcome' in df.columns or 'diagnosis' in df.columns:\n",
        "    group_col = 'Outcome' if 'Outcome' in df.columns else 'diagnosis'\n",
        "    try:\n",
        "        print(f\"\\nCluster vs {group_col} cross-tab:\")\n",
        "        ct = pd.crosstab(df['Cluster'], df[group_col], margins=True)\n",
        "        display(ct)\n",
        "    except Exception as e:\n",
        "        print(\"Error computing cluster vs outcome cross-tab:\", e)\n",
        "\n",
        "print(\"\\nK-Means clustering complete. Adjust CHOSEN_K or K_RANGE and re-run to explore other clusterings.\")"
      ],
      "metadata": {
        "id": "8QcICzOpk9uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apriori / FP-Growth (Colab-ready, fast & safe)\n",
        "# Paste this whole file into a Colab cell and run. It will prompt you to upload a CSV.\n",
        "# Designed to be equivalent to the \"apriori_vscode_fast.py\" desktop script but adapted\n",
        "# for Google Colab: uses files.upload(), files.download(), and installs mlxtend if needed.\n",
        "#\n",
        "# Key behaviours:\n",
        "#  - Auto-detects transactional datasets (Groceries / Online Retail) or converts tabular data\n",
        "#    (e.g., breast-cancer) into transactions by binning numeric columns.\n",
        "#  - Converts one-hot matrix to boolean dtype to satisfy mlxtend and speed up mining.\n",
        "#  - Prunes low-support item columns and optionally auto-reduces top-K items to avoid combinatorial explosion.\n",
        "#  - Uses fpgrowth by default (faster than apriori) and limits max itemset length for safety.\n",
        "#  - Saves frequent itemsets and association rules CSVs and downloads them automatically.\n",
        "#\n",
        "# Tweak PARAMETERS below before running if desired.\n",
        "\n",
        "# Install mlxtend if not installed (works in Colab)\n",
        "try:\n",
        "    import mlxtend\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    print(\"Installing mlxtend...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"mlxtend\"])\n",
        "    print(\"mlxtend installed.\")\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "\n",
        "# ---------------------- PARAMETERS ----------------------\n",
        "MIN_SUPPORT = 0.12       # relative support threshold (0..1)\n",
        "MIN_CONFIDENCE = 0.6    # minimum confidence for rules\n",
        "MIN_LIFT = 1.2          # minimum lift for \"strong\" rules\n",
        "MAX_LEN = 3             # maximum itemset length (reduce complexity)\n",
        "ALGO = \"fpgrowth\"       # \"fpgrowth\" (recommended) or \"apriori\"\n",
        "TOP_N = 20              # how many top itemsets/rules to display\n",
        "AUTO_REDUCE = True      # if True and many items, keep only TOP_K_ITEMS by support\n",
        "TOP_K_ITEMS = 100       # used when AUTO_REDUCE=True\n",
        "# ------------------------------------------------------\n",
        "\n",
        "print(\"Please upload your CSV file (e.g. Kaggle breast-cancer data.csv or diabetes.csv).\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Exiting.\")\n",
        "\n",
        "fname = next(iter(uploaded.keys()))\n",
        "print(f\"Loaded: {fname}\")\n",
        "\n",
        "# Read CSV into DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[fname]))\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"Failed to read uploaded CSV: {e}\")\n",
        "\n",
        "print(\"Initial shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Canonicalize common Kaggle breast-cancer quirks\n",
        "if 'Unnamed: 32' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 32'])\n",
        "    print(\"Dropped column: Unnamed: 32\")\n",
        "if 'id' in df.columns:\n",
        "    df = df.drop(columns=['id'])\n",
        "    print(\"Dropped column: id\")\n",
        "if 'diagnosis' in df.columns and 'Outcome' not in df.columns:\n",
        "    df['Outcome'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "    print(\"Mapped 'diagnosis' -> 'Outcome' (M->1, B->0)\")\n",
        "\n",
        "# Replace medically-invalid zeros for diabetes-like columns (if present)\n",
        "zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "zero_cols_present = [c for c in zero_not_allowed if c in df.columns]\n",
        "if zero_cols_present:\n",
        "    print(\"Replacing medically-invalid zeros with column means for:\", zero_cols_present)\n",
        "    for c in zero_cols_present:\n",
        "        df[c] = df[c].replace(0, np.nan)\n",
        "        df[c].fillna(df[c].mean(), inplace=True)\n",
        "\n",
        "# Detect transactional dataset formats\n",
        "is_groceries = {'Member_number', 'Date', 'ItemDescription'}.issubset(set(df.columns))\n",
        "is_online_retail = {'InvoiceNo', 'Description'}.issubset(set(df.columns))\n",
        "\n",
        "# Build one-hot transaction matrix\n",
        "if is_groceries:\n",
        "    print(\"Detected Groceries-style transactional dataset.\")\n",
        "    TRANSACTION_ID_COLUMNS = ['Member_number', 'Date']\n",
        "    ITEM_COLUMN = 'ItemDescription'\n",
        "    df = df.drop_duplicates(subset=TRANSACTION_ID_COLUMNS + [ITEM_COLUMN], keep='first')\n",
        "    basket_sets = (df.groupby(TRANSACTION_ID_COLUMNS)[ITEM_COLUMN]\n",
        "                  .apply(lambda x: pd.Series(1, index=x))\n",
        "                  .unstack(fill_value=0))\n",
        "    basket_sets.columns.name = None\n",
        "    basket_sets.index.names = TRANSACTION_ID_COLUMNS\n",
        "    transactions_ohe = basket_sets.applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "elif is_online_retail:\n",
        "    print(\"Detected Online Retail transactional dataset.\")\n",
        "    TRANSACTION_ID_COLUMNS = ['InvoiceNo']\n",
        "    ITEM_COLUMN = 'Description'\n",
        "    df = df.drop_duplicates(subset=TRANSACTION_ID_COLUMNS + [ITEM_COLUMN], keep='first')\n",
        "    basket_sets = (df.groupby(TRANSACTION_ID_COLUMNS)[ITEM_COLUMN]\n",
        "                  .apply(lambda x: pd.Series(1, index=x))\n",
        "                  .unstack(fill_value=0))\n",
        "    basket_sets.columns.name = None\n",
        "    basket_sets.index.names = TRANSACTION_ID_COLUMNS\n",
        "    transactions_ohe = basket_sets.applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "else:\n",
        "    # Tabular dataset (e.g., breast-cancer) -> convert rows to transactions by binning numeric columns\n",
        "    print(\"Non-transactional/tabular dataset detected. Binning numeric columns into Low/Med/High and using categorical columns.\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'Outcome' in numeric_cols:\n",
        "        numeric_cols.remove('Outcome')\n",
        "    if len(numeric_cols) == 0:\n",
        "        raise SystemExit(\"No numeric columns to bin; cannot create transactions for tabular dataset.\")\n",
        "\n",
        "    binned_cols = []\n",
        "    binary_cols = []\n",
        "    for col in numeric_cols:\n",
        "        try:\n",
        "            if df[col].nunique(dropna=True) <= 2:\n",
        "                df[col] = df[col].astype(str)\n",
        "                binary_cols.append(col)\n",
        "            else:\n",
        "                df[f\"{col}_bin\"] = pd.qcut(df[col].rank(method=\"first\"), q=3, labels=[\"Low\", \"Med\", \"High\"])\n",
        "                binned_cols.append(f\"{col}_bin\")\n",
        "        except Exception:\n",
        "            df[f\"{col}_bin\"] = pd.cut(df[col], bins=3, labels=[\"Low\", \"Med\", \"High\"])\n",
        "            binned_cols.append(f\"{col}_bin\")\n",
        "\n",
        "    cat_cols = [c for c in df.select_dtypes(include=['object', 'category']).columns.tolist() if c not in binary_cols]\n",
        "    cols_to_dummify = binned_cols + binary_cols + cat_cols\n",
        "    if not cols_to_dummify:\n",
        "        raise SystemExit(\"No columns available to create transactions (after binning). Aborting.\")\n",
        "\n",
        "    dummies = []\n",
        "    for col in cols_to_dummify:\n",
        "        s = df[col].astype(str)\n",
        "        d = pd.get_dummies(s, prefix=col)\n",
        "        dummies.append(d)\n",
        "    transactions_ohe = pd.concat(dummies, axis=1)\n",
        "    # convert to 0/1 ints\n",
        "    transactions_ohe = transactions_ohe.fillna(0).astype(int)\n",
        "\n",
        "print(\"One-hot transaction matrix shape (before pruning):\", transactions_ohe.shape)\n",
        "\n",
        "# Convert to boolean dtype for mlxtend (speeds up computation and avoids deprecation warnings)\n",
        "transactions_ohe = transactions_ohe.astype(bool)\n",
        "print(\"Converted transaction matrix to bool dtype.\")\n",
        "\n",
        "# Prune item columns whose absolute support < min_count (cannot be frequent)\n",
        "min_count = math.ceil(MIN_SUPPORT * transactions_ohe.shape[0])\n",
        "col_supports = transactions_ohe.sum(axis=0)\n",
        "low_support_cols = col_supports[col_supports < min_count].index.tolist()\n",
        "if len(low_support_cols) > 0:\n",
        "    transactions_ohe = transactions_ohe.drop(columns=low_support_cols)\n",
        "    print(f\"Dropped {len(low_support_cols)} item columns with support < min_count ({min_count})\")\n",
        "print(\"Final OHE shape (after low-support pruning):\", transactions_ohe.shape)\n",
        "\n",
        "# Auto-reduce if requested and items > TOP_K_ITEMS\n",
        "n_items = transactions_ohe.shape[1]\n",
        "if AUTO_REDUCE and n_items > TOP_K_ITEMS:\n",
        "    print(f\"AUTO_REDUCE engaged: {n_items} items > TOP_K_ITEMS ({TOP_K_ITEMS}). Keeping top-{TOP_K_ITEMS} by support.\")\n",
        "    item_supports = transactions_ohe.sum(axis=0).sort_values(ascending=False)\n",
        "    top_keep = item_supports.head(TOP_K_ITEMS).index.tolist()\n",
        "    transactions_ohe = transactions_ohe[top_keep]\n",
        "    print(\"Reduced OHE shape:\", transactions_ohe.shape)\n",
        "    n_items = transactions_ohe.shape[1]\n",
        "\n",
        "# Safety check and warning\n",
        "if n_items > 150:\n",
        "    print(\"\\nWARNING: Large number of item columns:\", n_items)\n",
        "    print(\"Mining may be slow or run out of memory. Consider increasing MIN_SUPPORT or using AUTO_REDUCE.\")\n",
        "    # continue, but user can interrupt if desired\n",
        "\n",
        "if transactions_ohe.shape[1] == 0:\n",
        "    raise SystemExit(\"No item columns remain after pruning. Try lowering MIN_SUPPORT.\")\n",
        "\n",
        "# Run frequent-pattern mining (fpgrowth preferred)\n",
        "print(f\"\\nRunning {ALGO} with min_support={MIN_SUPPORT}, max_len={MAX_LEN} ...\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    if ALGO.lower() == \"fpgrowth\":\n",
        "        freq_itemsets = fpgrowth(transactions_ohe, min_support=MIN_SUPPORT, use_colnames=True, max_len=MAX_LEN)\n",
        "    else:\n",
        "        freq_itemsets = apriori(transactions_ohe, min_support=MIN_SUPPORT, use_colnames=True, max_len=MAX_LEN)\n",
        "except Exception as e:\n",
        "    print(\"Frequent pattern mining failed:\", e)\n",
        "    # fallback attempt with higher support and smaller max_len\n",
        "    fallback_support = min(max(0.2, MIN_SUPPORT * 2), 0.95)\n",
        "    print(f\"Attempting fallback with higher min_support={fallback_support:.2f} and max_len=2 ...\")\n",
        "    try:\n",
        "        if ALGO.lower() == \"fpgrowth\":\n",
        "            freq_itemsets = fpgrowth(transactions_ohe, min_support=fallback_support, use_colnames=True, max_len=2)\n",
        "        else:\n",
        "            freq_itemsets = apriori(transactions_ohe, min_support=fallback_support, use_colnames=True, max_len=2)\n",
        "        print(\"Fallback mining succeeded.\")\n",
        "    except Exception as e2:\n",
        "        raise SystemExit(f\"Fallback also failed: {e2}\")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Mining completed in {elapsed:.2f}s. Found {len(freq_itemsets)} frequent itemsets.\")\n",
        "\n",
        "if freq_itemsets is None or freq_itemsets.empty:\n",
        "    print(\"No frequent itemsets found. Try lowering MIN_SUPPORT or increasing MAX_LEN.\")\n",
        "else:\n",
        "    freq_itemsets['length'] = freq_itemsets['itemsets'].apply(lambda s: len(s))\n",
        "    freq_itemsets = freq_itemsets.sort_values(['support','length'], ascending=[False, False]).reset_index(drop=True)\n",
        "    freq_itemsets['readable'] = freq_itemsets['itemsets'].apply(lambda s: \", \".join(sorted([str(i) for i in s])))\n",
        "    out_freq = \"frequent_itemsets_colab_fast.csv\"\n",
        "    freq_itemsets.to_csv(out_freq, index=False)\n",
        "    print(f\"Saved frequent itemsets to: {out_freq}\")\n",
        "    display(freq_itemsets.head(TOP_N)[['readable','support','length']])\n",
        "\n",
        "# Generate association rules if itemsets present\n",
        "if 'freq_itemsets' in locals() and not freq_itemsets.empty:\n",
        "    print(f\"\\nGenerating association rules with min_confidence={MIN_CONFIDENCE} ...\")\n",
        "    rules = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
        "    if rules is None or rules.empty:\n",
        "        print(\"No rules generated at this MIN_CONFIDENCE. Consider lowering MIN_CONFIDENCE.\")\n",
        "    else:\n",
        "        strong_rules = rules[rules['lift'] >= MIN_LIFT].copy()\n",
        "        strong_rules = strong_rules.sort_values(['confidence','lift','support'], ascending=[False, False, False]).reset_index(drop=True)\n",
        "        strong_rules['antecedent_readable'] = strong_rules['antecedents'].apply(lambda s: \", \".join(sorted([str(i) for i in s])))\n",
        "        strong_rules['consequent_readable'] = strong_rules['consequents'].apply(lambda s: \", \".join(sorted([str(i) for i in s])))\n",
        "        out_rules = \"association_rules_colab_fast.csv\"\n",
        "        strong_rules.to_csv(out_rules, index=False)\n",
        "        print(f\"Saved filtered association rules to: {out_rules}\")\n",
        "        if not strong_rules.empty:\n",
        "            display_cols = ['antecedent_readable','consequent_readable','support','confidence','lift']\n",
        "            display(strong_rules.head(TOP_N)[display_cols].round(3))\n",
        "else:\n",
        "    print(\"Skipping rule generation because no frequent itemsets were found.\")\n",
        "\n",
        "# Plot rules scatter (support vs confidence colored by lift) if rules exist\n",
        "if 'rules' in locals() and rules is not None and not rules.empty:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    scatter = plt.scatter(rules['support'], rules['confidence'], c=rules['lift'],\n",
        "                          s=(rules['lift']*40).clip(10,300), cmap='viridis', alpha=0.75, edgecolor='k', linewidth=0.3)\n",
        "    plt.xlabel(\"Support\")\n",
        "    plt.ylabel(\"Confidence\")\n",
        "    plt.title(\"Association Rules: Confidence vs Support (Color = Lift)\")\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('Lift')\n",
        "    plt.axhline(MIN_CONFIDENCE, color='r', linestyle='--', linewidth=1, label=f'Min Confidence ({MIN_CONFIDENCE})')\n",
        "    plt.axvline(MIN_SUPPORT, color='b', linestyle='--', linewidth=1, label=f'Min Support ({MIN_SUPPORT})')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    plot_file = \"apriori_rules_visualization_colab_fast.png\"\n",
        "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Saved rules scatter plot to: {plot_file}\")\n",
        "    try:\n",
        "        files.download(out_freq)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        files.download(out_rules)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        files.download(plot_file)\n",
        "    except Exception:\n",
        "        pass\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No rule plot created because no rules were generated.\")\n",
        "\n",
        "print(\"\\nDone. Files saved to notebook filesystem and (where possible) downloaded.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GT6l0s4Cp0ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales Data Warehouse ETL - Based on Star Schema\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# Step 1: EXTRACT - Raw Sales Data\n",
        "# ======================================================\n",
        "\n",
        "# Time Dimension\n",
        "dim_time = pd.DataFrame({\n",
        "    'time_id': [1, 2, 3, 4, 5],\n",
        "    'date': pd.to_datetime(['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05', '2024-05-12']),\n",
        "    'day': [15, 20, 10, 5, 12],\n",
        "    'month': [1, 2, 3, 4, 5],\n",
        "    'year': [2024, 2024, 2024, 2024, 2024]\n",
        "})\n",
        "\n",
        "# Sales Channel Dimension\n",
        "dim_sales_channel = pd.DataFrame({\n",
        "    'channel_id': [1, 2],\n",
        "    'channel_name': ['online', 'in-store'],\n",
        "    'channel_type': ['digital', 'physical']\n",
        "})\n",
        "\n",
        "# Customer Dimension\n",
        "dim_customer = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4, 5],\n",
        "    'name': ['John Smith', 'Sarah Johnson', 'Mike Davis', 'Emily Wilson', 'David Brown'],\n",
        "    'age': [28, 35, 42, 31, 29],\n",
        "    'gender': ['M', 'F', 'M', 'F', 'M'],\n",
        "    'segment': ['Premium', 'Standard', 'Premium', 'Standard', 'Premium']\n",
        "})\n",
        "\n",
        "# Region Dimension\n",
        "dim_region = pd.DataFrame({\n",
        "    'region_id': [1, 2, 3],\n",
        "    'name': ['North America', 'Europe', 'Asia'],\n",
        "    'country': ['USA', 'Germany', 'Japan'],\n",
        "    'state': ['California', 'Bavaria', 'Tokyo'],\n",
        "    'city': ['Los Angeles', 'Munich', 'Tokyo']\n",
        "})\n",
        "\n",
        "# Product Dimension\n",
        "dim_product = pd.DataFrame({\n",
        "    'product_id': [1, 2, 3, 4],\n",
        "    'name': ['Laptop Pro', 'Smartphone X', 'Tablet Plus', 'Headphones'],\n",
        "    'category': ['Electronics', 'Electronics', 'Electronics', 'Accessories'],\n",
        "    'price': [1200.00, 800.00, 600.00, 150.00],\n",
        "    'supplier': ['TechCorp', 'MobileTech', 'TabletInc', 'AudioMax']\n",
        "})\n",
        "\n",
        "# Sales Fact Table (Raw Data)\n",
        "fact_sales_raw = pd.DataFrame({\n",
        "    'sale_id': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'time_id': [1, 2, 3, 4, 5, 1, 2, 3],\n",
        "    'channel_id': [1, 2, 1, 2, 1, 2, 1, 2],\n",
        "    'customer_id': [1, 2, 3, 4, 5, 2, 1, 3],\n",
        "    'region_id': [1, 2, 3, 1, 2, 1, 3, 2],\n",
        "    'product_id': [1, 2, 3, 4, 1, 2, 3, 4],\n",
        "    'no_of_transactions': [1, 2, 1, 3, 1, 2, 1, 1],\n",
        "    'discount_offered': [100.00, 50.00, 0.00, 20.00, 150.00, 75.00, 30.00, 10.00]\n",
        "})\n",
        "\n",
        "# ======================================================\n",
        "# Step 2: TRANSFORM - Calculate Derived Fields\n",
        "# ======================================================\n",
        "\n",
        "# Add product price to sales data for total sales calculation\n",
        "fact_sales = fact_sales_raw.merge(dim_product[['product_id', 'price']], on='product_id')\n",
        "\n",
        "# Calculate total sales (price * quantity - discount)\n",
        "fact_sales['total_sales'] = (fact_sales['price'] * fact_sales['no_of_transactions']) - fact_sales['discount_offered']\n",
        "\n",
        "# Final fact table with required columns\n",
        "fact_sales_final = fact_sales[['sale_id', 'time_id', 'channel_id', 'customer_id', 'region_id', 'product_id',\n",
        "                               'total_sales', 'no_of_transactions', 'discount_offered']]\n",
        "\n",
        "print(\"Sample Data Preview:\")\n",
        "print(\"\\n Time Dimension:\")\n",
        "print(dim_time.head())\n",
        "print(\"\\n Sales Channel Dimension:\")\n",
        "print(dim_sales_channel)\n",
        "print(\"\\n Customer Dimension:\")\n",
        "print(dim_customer.head())\n",
        "print(\"\\n Sales Fact Table:\")\n",
        "print(fact_sales_final.head())\n",
        "\n",
        "# ======================================================\n",
        "# Step 3: LOAD - Store in SQLite Data Warehouse\n",
        "# ======================================================\n",
        "\n",
        "conn = sqlite3.connect('sales_dw.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Save each dimension and fact table\n",
        "dim_time.to_sql('dim_time', conn, if_exists='replace', index=False)\n",
        "dim_sales_channel.to_sql('dim_sales_channel', conn, if_exists='replace', index=False)\n",
        "dim_customer.to_sql('dim_customer', conn, if_exists='replace', index=False)\n",
        "dim_region.to_sql('dim_region', conn, if_exists='replace', index=False)\n",
        "dim_product.to_sql('dim_product', conn, if_exists='replace', index=False)\n",
        "fact_sales_final.to_sql('fact_sales', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"\\n Sales ETL completed successfully. Data loaded into 'sales_dw.db'.\")\n",
        "print(f\" Database contains {len(fact_sales_final)} sales transactions\")\n",
        "print(f\" Spanning {len(dim_time)} time periods, {len(dim_customer)} customers, and {len(dim_product)} products\")\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "WtU1PAzBEMZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# Step 4: VIEW SALES DATABASE CONTENTS\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "# Check if database file exists and show its location\n",
        "db_path = 'sales_dw.db'\n",
        "if os.path.exists(db_path):\n",
        "    print(f\" Sales Database found at: {os.path.abspath(db_path)}\")\n",
        "    print(f\" File size: {os.path.getsize(db_path)} bytes\")\n",
        "else:\n",
        "    print(\" Sales Database not found. Run the ETL code first!\")\n",
        "\n",
        "# Connect and display all tables\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Show all tables in the database\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" TABLES IN SALES DATA WAREHOUSE:\")\n",
        "print(\"=\"*60)\n",
        "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = pd.read_sql_query(tables_query, conn)\n",
        "print(tables.to_string(index=False))\n",
        "\n",
        "# Display contents of each table\n",
        "table_names = ['dim_time', 'dim_sales_channel', 'dim_customer', 'dim_region', 'dim_product', 'fact_sales']\n",
        "\n",
        "for table in table_names:\n",
        "    try:\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\" CONTENTS OF {table.upper()}:\")\n",
        "        print(\"=\"*60)\n",
        "        df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
        "        print(df.to_string(index=False))\n",
        "        print(f\"\\n Statistics: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "        # Show some key stats for fact table\n",
        "        if table == 'fact_sales':\n",
        "            total_sales = df['total_sales'].sum()\n",
        "            avg_sales = df['total_sales'].mean()\n",
        "            print(f\" Total Sales: ${total_sales:,.2f}\")\n",
        "            print(f\" Average Sale: ${avg_sales:,.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Could not read {table}: {e}\")\n",
        "\n",
        "conn.close()\n",
        "print(\"\\n Sales Database view completed!\")"
      ],
      "metadata": {
        "id": "twTTqyx9Gji0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales Data Warehouse OLAP Analysis\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "# Connect to Sales Data Warehouse\n",
        "conn = sqlite3.connect('sales_dw.db')\n",
        "\n",
        "# Helper function to display query results neatly\n",
        "def show_query(title, query):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" {title}\")\n",
        "    print('='*60)\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    print(df.to_string(index=False))\n",
        "    if 'total_sales' in df.columns or 'Total_Sales' in df.columns:\n",
        "        sales_col = 'total_sales' if 'total_sales' in df.columns else 'Total_Sales'\n",
        "        print(f\"\\n Total Sales: ${df[sales_col].sum():,.2f}\")\n",
        "\n",
        "# ======================================================\n",
        "# SALES OLAP OPERATIONS\n",
        "# ======================================================\n",
        "\n",
        "# (a) ROLL-UP: Aggregate sales by region\n",
        "query_rollup = \"\"\"\n",
        "SELECT r.name as Region, SUM(s.total_sales) AS Total_Sales\n",
        "FROM fact_sales s\n",
        "JOIN dim_region r ON s.region_id = r.region_id\n",
        "GROUP BY r.name\n",
        "ORDER BY Total_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"ROLL-UP: Sales by Region\", query_rollup)\n",
        "\n",
        "# (b) DRILL-DOWN: Region  Country  City\n",
        "query_drilldown = \"\"\"\n",
        "SELECT r.name as Region, r.country, r.city, SUM(s.total_sales) AS City_Sales\n",
        "FROM fact_sales s\n",
        "JOIN dim_region r ON s.region_id = r.region_id\n",
        "GROUP BY r.name, r.country, r.city\n",
        "ORDER BY City_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"DRILL-DOWN: Sales by Region  Country  City\", query_drilldown)\n",
        "\n",
        "# (c) SLICE: Filter by sales channel (online only)\n",
        "query_slice = \"\"\"\n",
        "SELECT p.name as Product, SUM(s.total_sales) AS Online_Sales,\n",
        "       SUM(s.no_of_transactions) as Transactions\n",
        "FROM fact_sales s\n",
        "JOIN dim_product p ON s.product_id = p.product_id\n",
        "JOIN dim_sales_channel c ON s.channel_id = c.channel_id\n",
        "WHERE c.channel_name = 'online'\n",
        "GROUP BY p.name\n",
        "ORDER BY Online_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"SLICE: Online Sales by Product\", query_slice)\n",
        "\n",
        "# (d) DICE: Filter by Premium customers AND Electronics category\n",
        "query_dice = \"\"\"\n",
        "SELECT r.name as Region, p.name as Product, SUM(s.total_sales) AS Premium_Electronics_Sales\n",
        "FROM fact_sales s\n",
        "JOIN dim_customer c ON s.customer_id = c.customer_id\n",
        "JOIN dim_product p ON s.product_id = p.product_id\n",
        "JOIN dim_region r ON s.region_id = r.region_id\n",
        "WHERE c.segment = 'Premium' AND p.category = 'Electronics'\n",
        "GROUP BY r.name, p.name\n",
        "ORDER BY Premium_Electronics_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"DICE: Premium Customer Electronics Sales by Region\", query_dice)\n",
        "\n",
        "# (e) PIVOT: Sales Channel vs Product Category\n",
        "query_pivot = \"\"\"\n",
        "SELECT p.category as Product_Category,\n",
        "  SUM(CASE WHEN c.channel_name = 'online' THEN s.total_sales ELSE 0 END) AS Online_Sales,\n",
        "  SUM(CASE WHEN c.channel_name = 'in-store' THEN s.total_sales ELSE 0 END) AS InStore_Sales,\n",
        "  SUM(s.total_sales) AS Total_Sales\n",
        "FROM fact_sales s\n",
        "JOIN dim_product p ON s.product_id = p.product_id\n",
        "JOIN dim_sales_channel c ON s.channel_id = c.channel_id\n",
        "GROUP BY p.category\n",
        "ORDER BY Total_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"PIVOT: Sales Channel vs Product Category\", query_pivot)\n",
        "\n",
        "# (f) TIME ANALYSIS: Monthly sales trend\n",
        "query_time = \"\"\"\n",
        "SELECT t.month, t.year, SUM(s.total_sales) AS Monthly_Sales,\n",
        "       COUNT(s.sale_id) AS Total_Transactions,\n",
        "       AVG(s.total_sales) AS Avg_Sale_Value\n",
        "FROM fact_sales s\n",
        "JOIN dim_time t ON s.time_id = t.time_id\n",
        "GROUP BY t.year, t.month\n",
        "ORDER BY t.year, t.month;\n",
        "\"\"\"\n",
        "show_query(\"TIME ANALYSIS: Monthly Sales Trend\", query_time)\n",
        "\n",
        "# (g) CUSTOMER ANALYSIS: Top customers by sales\n",
        "query_customers = \"\"\"\n",
        "SELECT c.name as Customer, c.segment, SUM(s.total_sales) AS Customer_Sales,\n",
        "       COUNT(s.sale_id) as Purchases, AVG(s.total_sales) as Avg_Purchase\n",
        "FROM fact_sales s\n",
        "JOIN dim_customer c ON s.customer_id = c.customer_id\n",
        "GROUP BY c.customer_id, c.name, c.segment\n",
        "ORDER BY Customer_Sales DESC;\n",
        "\"\"\"\n",
        "show_query(\"CUSTOMER ANALYSIS: Top Customers\", query_customers)\n",
        "\n",
        "# ======================================================\n",
        "# CLOSE CONNECTION\n",
        "# ======================================================\n",
        "conn.close()\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\" Sales OLAP analysis completed successfully!\")\n",
        "print(\" Analysis included: Roll-up, Drill-down, Slice, Dice, Pivot, Time & Customer Analysis\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "Mu4W7xgsGpYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}